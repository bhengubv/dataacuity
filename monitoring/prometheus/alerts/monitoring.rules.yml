groups:
  - name: infrastructure_alerts
    interval: 30s
    rules:
      # ==========================================
      # System Resource Alerts
      # ==========================================

      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High CPU usage detected on {{ $labels.instance }}"
          description: "CPU usage is above 80% (current: {{ $value | humanize }}%)"

      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 2m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "CRITICAL: CPU usage on {{ $labels.instance }}"
          description: "CPU usage is critically high at {{ $value | humanize }}%"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 80% (current: {{ $value | humanize }}%)"

      - alert: CriticalMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 2m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "CRITICAL: Memory exhausted on {{ $labels.instance }}"
          description: "Memory usage is critically high at {{ $value | humanize }}%"

      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 20
        for: 5m
        labels:
          severity: warning
          component: storage
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk space is below 20% ({{ $value | humanize }}% remaining)"

      - alert: DiskSpaceCritical
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 10
        for: 2m
        labels:
          severity: critical
          component: storage
        annotations:
          summary: "CRITICAL: Disk space nearly full on {{ $labels.instance }}"
          description: "Only {{ $value | humanize }}% disk space remaining"

  - name: container_alerts
    interval: 30s
    rules:
      # ==========================================
      # Docker Container Alerts
      # ==========================================

      - alert: ContainerDown
        expr: up{job=~".*exporter"} == 0
        for: 1m
        labels:
          severity: critical
          component: container
        annotations:
          summary: "Container {{ $labels.job }} is down"
          description: "The {{ $labels.job }} container on {{ $labels.instance }} has been down for more than 1 minute"

      - alert: HighContainerMemory
        expr: |
          (container_memory_usage_bytes{name!=""} / container_spec_memory_limit_bytes{name!=""}) * 100 > 90
          and container_spec_memory_limit_bytes{name!=""} > 0
        for: 5m
        labels:
          severity: warning
          component: container
        annotations:
          summary: "High memory usage in container {{ $labels.name }}"
          description: "Container is using {{ $value | humanize }}% of its memory limit"

      - alert: ContainerCPUThrottling
        expr: rate(container_cpu_cfs_throttled_seconds_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
          component: container
        annotations:
          summary: "Container {{ $labels.name }} is being CPU throttled"
          description: "Container CPU is being throttled ({{ $value | humanize }} seconds over 5 minutes)"

  - name: database_alerts
    interval: 30s
    rules:
      # ==========================================
      # PostgreSQL Database Alerts
      # ==========================================

      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "PostgreSQL database {{ $labels.instance }} is down"
          description: "Cannot connect to PostgreSQL database"

      - alert: PostgreSQLTooManyConnections
        expr: sum by(instance) (pg_stat_activity_count) > 80
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "High number of connections to {{ $labels.instance }}"
          description: "Database has {{ $value }} active connections"

      - alert: PostgreSQLSlowQueries
        expr: rate(pg_stat_activity_max_tx_duration[5m]) > 60
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Slow queries detected on {{ $labels.instance }}"
          description: "Long-running queries detected ({{ $value | humanize }} seconds)"

  - name: redis_alerts
    interval: 30s
    rules:
      # ==========================================
      # Redis Cache Alerts
      # ==========================================

      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          component: cache
        annotations:
          summary: "Redis instance {{ $labels.instance }} is down"
          description: "Cannot connect to Redis cache"

      - alert: RedisHighMemory
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 90
          and redis_memory_max_bytes > 0
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Redis memory usage high on {{ $labels.instance }}"
          description: "Redis is using {{ $value | humanize }}% of maximum memory"

      - alert: RedisTooManyConnections
        expr: redis_connected_clients > 100
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Too many Redis connections on {{ $labels.instance }}"
          description: "Redis has {{ $value }} connected clients"

  - name: application_alerts
    interval: 30s
    rules:
      # ==========================================
      # Application & API Alerts
      # ==========================================

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100 > 5
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "Error rate is {{ $value | humanize }}% over the last 5 minutes"

      - alert: CriticalErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100 > 20
        for: 2m
        labels:
          severity: critical
          component: application
        annotations:
          summary: "CRITICAL: Very high error rate on {{ $labels.job }}"
          description: "Error rate is {{ $value | humanize }}% - investigate immediately"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job)) > 2
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High response time on {{ $labels.job }}"
          description: "95th percentile response time is {{ $value | humanize }} seconds"

      - alert: LowRequestRate
        expr: rate(http_requests_total[5m]) < 0.01
        for: 10m
        labels:
          severity: info
          component: application
        annotations:
          summary: "Low request rate on {{ $labels.job }}"
          description: "Request rate has dropped to {{ $value | humanize }} req/s - possible issue?"

  - name: monitoring_stack_alerts
    interval: 30s
    rules:
      # ==========================================
      # Monitoring Stack Health
      # ==========================================

      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "Prometheus is down"
          description: "The Prometheus monitoring service is not responding"

      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 1m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Grafana is down"
          description: "The Grafana dashboard service is not responding"

      - alert: PrometheusTSDBCompactionsFailing
        expr: rate(prometheus_tsdb_compactions_failed_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Prometheus TSDB compactions failing"
          description: "TSDB compaction failures detected ({{ $value | humanize }})"

      - alert: PrometheusTargetDown
        expr: up == 0
        for: 2m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Prometheus target {{ $labels.job }} is down"
          description: "Target {{ $labels.instance }} has been down for more than 2 minutes"
